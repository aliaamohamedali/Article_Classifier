{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OKUser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1e74e42d1f58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk_stop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'English'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# nltk_stop_words = set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "TRAIN_DATA_PATH = 'bbc_train'\n",
    "TEST_DATA_PATH = 'bbc_test'\n",
    "DELIMITERS = [',', '.', '!', '?', '/', '&', '-', ':', ';', '@', '\"', \"'\"]\n",
    "STOP_WORDS = open('stopwords.txt').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL VARIABLES\n",
    "\n",
    "## Constants\n",
    "# Optionally use, for the purpose of this I\n",
    "# hardcoded the values, for use with different\n",
    "# datasets they are calculated as P(cj) = num(cj)/total(C) during training\n",
    "article_types = []#['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "article_type_probabilities ={\n",
    "  'business': 450/1979,\n",
    "  'entertainment': 344/1979,\n",
    "  'politics': 375/1979,\n",
    "  'sport': 454/1979,\n",
    "  'tech': 356/1979  \n",
    "}\n",
    "\n",
    "## PARAMETERS COULD BE TUNED\n",
    "MOST_COMMON_TRAIN = 210\n",
    "MOST_COMMON_TEST = 10\n",
    "# Laplace smoothing factor Set to 1, change by inspection\n",
    "ALPHA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MORE GLOBAL VARIABLES\n",
    "\n",
    "# Dictionary of word frequencies of eavh article type\n",
    "# Key = Article Type\n",
    "# Value = tuple of word lists and their frequencies\n",
    "# Value is created by the counter() function which\n",
    "# creates a list of elements and their numbers\n",
    "article_types_word_frequencies = {}\n",
    "# total vocabulary of all articles\n",
    "\n",
    "### Moved locally\n",
    "vocabulary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(train_data_path):\n",
    "    \n",
    "    global vocabulary\n",
    "    global article_types_word_frequencies\n",
    "    \n",
    "    for article_type in os.listdir(train_data_path):\n",
    "        #print(article_type)\n",
    "        # Add article types to list\n",
    "        article_types.append(article_type)\n",
    "        article_type_path = path.join(train_data_path, article_type)\n",
    "        article_type_words = []\n",
    "        for article in os.listdir(article_type_path):\n",
    "            #print(article)\n",
    "            article_path = path.join(article_type_path, article)\n",
    "            article_text = open(article_path).read()\n",
    "            words = article_text.split()\n",
    "            article_type_words.extend(''.join(w for w in word.lower() if w not in DELIMITERS) for word in words)\n",
    "        article_type_counter = Counter(article_type_words)        \n",
    "        # Remove null pointers generated from removing delimiters\n",
    "        article_type_counter.pop('', None)\n",
    "        # Remove stopping words\n",
    "        for stop_word in STOP_WORDS: \n",
    "            article_type_counter.pop(stop_word, None)\n",
    "            \n",
    "        article_types_word_frequencies[article_type] = article_type_counter.most_common(MOST_COMMON_TRAIN)\n",
    "        for word, freq in article_type_counter.most_common(MOST_COMMON_TRAIN):\n",
    "            vocabulary.extend([word])\n",
    "    # Remove duplicates - Don't care about order\n",
    "    vocabulary = list(set(vocabulary))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for some stuff\n",
    "print(len(vocabulary))\n",
    "for word in vocabulary:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save values of (n) of each article type\n",
    "n_article_type = {}\n",
    "for article_type in article_types:\n",
    "    n = 0\n",
    "    for word_freq_pair in article_types_word_frequencies[article_type]:\n",
    "        n += word_freq_pair[1]\n",
    "    n_article_type[article_type] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also Checking\n",
    "for article_type in article_types:\n",
    "    print('>>', article_type, n_article_type[article_type]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still Checking\n",
    "for article_type in article_types:\n",
    "    print('>>', article_type, n_article_type[article_type])\n",
    "    for word_freq_pair in article_types_word_frequencies[article_type]:        \n",
    "        print(word_freq_pair[0], word_freq_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency_given_article_type(word, article_type):\n",
    "    # Number of occurrences of specified word\n",
    "    nk = 0\n",
    "    # Number of words in specified article type\n",
    "    # n = 0\n",
    "    #n = n_article_type[article_type]\n",
    "    for word_freq_pair in article_types_word_frequencies[article_type]:\n",
    "        if word == word_freq_pair[0]:\n",
    "            nk = word_freq_pair[1]     \n",
    "        # n += word_freq_pair[1]    \n",
    "    # probability = (nk + ALPHA) / (n + ALPHA*len(vocabulary)) \n",
    "    # Should maybe save probabilites ??\n",
    "    return nk+ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save probabilities of each article type words\n",
    "# A dictionary of dictionaries\n",
    "# Key 1: article type\n",
    "# key 2: word\n",
    "# Value = probability of word given article type\n",
    "\n",
    "word_frequencies = {}\n",
    "for article_type in article_types:\n",
    "    word_frequencies[article_type] = {}\n",
    "    for word in vocabulary:\n",
    "        word_frequencies[article_type][word] = get_word_frequency_given_article_type(word, article_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_article(article_path):\n",
    "    \n",
    "    #print(article_path)\n",
    "    global article_type_probabilities\n",
    "    global article_types\n",
    "    \n",
    "    test_article_words = []\n",
    "    # May need this during analysis\n",
    "    #test_article_type_probabilities = []\n",
    "    max_prob = -1000000\n",
    "    likeliest_article_type = ''\n",
    "    \n",
    "    article_text = open(article_path).read()\n",
    "    words = article_text.split()\n",
    "    test_article_words.extend(''.join(w for w in word.lower() if w not in DELIMITERS) for word in words)\n",
    "    article_counter = Counter(test_article_words)\n",
    "    article_counter.pop('', None)\n",
    "    for stop_word in STOP_WORDS: \n",
    "        article_counter.pop(stop_word, None)\n",
    "        \n",
    "    article_words = article_counter.most_common(MOST_COMMON_TEST)\n",
    "    \n",
    "    for article_type in article_types:\n",
    "        # Will sum log(prob) to avoid small probabilities\n",
    "        article_type_probability = article_type_probabilities[article_type]\n",
    "        for word in test_article_words:\n",
    "            word_prob = math.log(word_frequencies[article_type].get(word, ALPHA)) - math.log(n_article_type[article_type] + ALPHA*len(vocabulary))\n",
    "            article_type_probability += word_prob     \n",
    "        #print('log: ', article_type, article_type_probability)\n",
    "        #article_type_probability = math.exp(article_type_probability)\n",
    "        #print('exp: ', article_type, article_type_probability)\n",
    "        if(article_type_probability > max_prob):\n",
    "            max_prob = article_type_probability\n",
    "            likeliest_article_type = article_type\n",
    "            \n",
    "    return  likeliest_article_type\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(test_data_path):\n",
    "    total_predictions = 0\n",
    "    total_correct_predictions = 0\n",
    "    \n",
    "    for article_type in os.listdir(test_data_path):\n",
    "        article_type_predictions = 0\n",
    "        article_type_correct_predictions = 0\n",
    "        print ('Articles of type:', article_type)\n",
    "        article_type_path = path.join(test_data_path, article_type)\n",
    "        for article in os.listdir(article_type_path):\n",
    "            article_type_predictions += 1\n",
    "            article_path = path.join(article_type_path, article)\n",
    "            article_prediction = predict_test_article(article_path)\n",
    "            print (article, 'Prediction:', article_prediction)\n",
    "            if article_prediction == article_type:\n",
    "                article_type_correct_predictions += 1\n",
    "        print ('Number of predictions: ', article_type_predictions)\n",
    "        print ('Correct predictions: ', article_type_correct_predictions)\n",
    "        print ('Accuracy: ', float(article_type_correct_predictions)/article_type_predictions)\n",
    "        total_predictions += article_type_predictions\n",
    "        total_correct_predictions += article_type_correct_predictions\n",
    "                 \n",
    "    print ('Total Number of predictions: ', total_predictions)\n",
    "    print ('Correct predictions: ', total_correct_predictions)\n",
    "    print ('Accuracy: ', float(total_correct_predictions)/total_predictions)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
